{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUiPKIg2mjdu"
   },
   "source": [
    "In this notebook, we trained the CNN-RNN model.  \n",
    "\n",
    "INDEX\n",
    "- [Step 1](#step1): Setting up the training phase\n",
    "- [Step 2](#step2): Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uUr12YYmjdw"
   },
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, we will customize the training of our CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "We begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days.\n",
    "- `save_every` - determines how often to save the model weights.  We set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oP_A_PbUmjdw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-Gz8j6imjdx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "import math\n",
    "\n",
    "batch_size = 32 #32 #64        # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = False   # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3           # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRd3XtHBmjdx",
    "outputId": "b58da143-7216-43f1-dee2-6818e63c9be0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.10s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.98s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1146/414113 [00:00<01:11, 5742.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:14<00:00, 5579.77it/s]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "from data_loader import get_loader\n",
    "\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters())\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRhhhMWOmjdy"
   },
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Training the Model\n",
    "\n",
    "Once the above cells have been executed successfully in **Step 1**, we will start the training now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmtVN8ynmjdy",
    "outputId": "22f4d194-4e0b-4d9c-f369-dd4438ba1e0f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/3236], Loss: 3.6071, Perplexity: 36.8589\n",
      "Epoch [1/3], Step [200/3236], Loss: 3.1628, Perplexity: 23.6371\n",
      "Epoch [1/3], Step [300/3236], Loss: 3.4074, Perplexity: 30.1859\n",
      "Epoch [1/3], Step [400/3236], Loss: 3.1240, Perplexity: 22.7376\n",
      "Epoch [1/3], Step [500/3236], Loss: 2.9587, Perplexity: 19.2733\n",
      "Epoch [1/3], Step [600/3236], Loss: 2.9260, Perplexity: 18.6520\n",
      "Epoch [1/3], Step [700/3236], Loss: 2.7981, Perplexity: 16.4135\n",
      "Epoch [1/3], Step [800/3236], Loss: 2.7010, Perplexity: 14.8949\n",
      "Epoch [1/3], Step [900/3236], Loss: 2.5678, Perplexity: 13.0366\n",
      "Epoch [1/3], Step [1000/3236], Loss: 2.4018, Perplexity: 11.0432\n",
      "Epoch [1/3], Step [1100/3236], Loss: 2.3271, Perplexity: 10.2480\n",
      "Epoch [1/3], Step [1200/3236], Loss: 2.4550, Perplexity: 11.6462\n",
      "Epoch [1/3], Step [1300/3236], Loss: 2.3690, Perplexity: 10.6865\n",
      "Epoch [1/3], Step [1400/3236], Loss: 2.4791, Perplexity: 11.9307\n",
      "Epoch [1/3], Step [1500/3236], Loss: 2.4279, Perplexity: 11.3354\n",
      "Epoch [1/3], Step [1600/3236], Loss: 2.2051, Perplexity: 9.07169\n",
      "Epoch [1/3], Step [1700/3236], Loss: 2.7174, Perplexity: 15.1405\n",
      "Epoch [1/3], Step [1800/3236], Loss: 2.2204, Perplexity: 9.21079\n",
      "Epoch [1/3], Step [1900/3236], Loss: 2.3524, Perplexity: 10.5109\n",
      "Epoch [1/3], Step [2000/3236], Loss: 2.3198, Perplexity: 10.1736\n",
      "Epoch [1/3], Step [2100/3236], Loss: 2.2882, Perplexity: 9.85700\n",
      "Epoch [1/3], Step [2200/3236], Loss: 2.1655, Perplexity: 8.71880\n",
      "Epoch [1/3], Step [2300/3236], Loss: 2.2705, Perplexity: 9.68398\n",
      "Epoch [1/3], Step [2400/3236], Loss: 2.7277, Perplexity: 15.2975\n",
      "Epoch [1/3], Step [2500/3236], Loss: 2.3608, Perplexity: 10.5995\n",
      "Epoch [1/3], Step [2600/3236], Loss: 2.4419, Perplexity: 11.4944\n",
      "Epoch [1/3], Step [2700/3236], Loss: 2.2918, Perplexity: 9.89324\n",
      "Epoch [1/3], Step [2800/3236], Loss: 2.6166, Perplexity: 13.6891\n",
      "Epoch [1/3], Step [2900/3236], Loss: 2.0716, Perplexity: 7.93749\n",
      "Epoch [1/3], Step [3000/3236], Loss: 2.0396, Perplexity: 7.68721\n",
      "Epoch [1/3], Step [3100/3236], Loss: 2.1491, Perplexity: 8.57695\n",
      "Epoch [1/3], Step [3200/3236], Loss: 2.0501, Perplexity: 7.76908\n",
      "Epoch [2/3], Step [100/3236], Loss: 2.3890, Perplexity: 10.90248\n",
      "Epoch [2/3], Step [200/3236], Loss: 2.3292, Perplexity: 10.2695\n",
      "Epoch [2/3], Step [300/3236], Loss: 2.2261, Perplexity: 9.26417\n",
      "Epoch [2/3], Step [400/3236], Loss: 2.3327, Perplexity: 10.3053\n",
      "Epoch [2/3], Step [500/3236], Loss: 2.0660, Perplexity: 7.89345\n",
      "Epoch [2/3], Step [600/3236], Loss: 2.1040, Perplexity: 8.19881\n",
      "Epoch [2/3], Step [700/3236], Loss: 2.0659, Perplexity: 7.89257\n",
      "Epoch [2/3], Step [800/3236], Loss: 2.2774, Perplexity: 9.75117\n",
      "Epoch [2/3], Step [900/3236], Loss: 2.1137, Perplexity: 8.27865\n",
      "Epoch [2/3], Step [1000/3236], Loss: 2.1685, Perplexity: 8.7453\n",
      "Epoch [2/3], Step [1100/3236], Loss: 2.1937, Perplexity: 8.96805\n",
      "Epoch [2/3], Step [1200/3236], Loss: 1.9836, Perplexity: 7.26924\n",
      "Epoch [2/3], Step [1300/3236], Loss: 2.0211, Perplexity: 7.54672\n",
      "Epoch [2/3], Step [1400/3236], Loss: 2.0069, Perplexity: 7.44044\n",
      "Epoch [2/3], Step [1500/3236], Loss: 2.3492, Perplexity: 10.4772\n",
      "Epoch [2/3], Step [1600/3236], Loss: 2.0108, Perplexity: 7.46942\n",
      "Epoch [2/3], Step [1700/3236], Loss: 2.1281, Perplexity: 8.39916\n",
      "Epoch [2/3], Step [1800/3236], Loss: 2.2793, Perplexity: 9.76986\n",
      "Epoch [2/3], Step [1900/3236], Loss: 2.4866, Perplexity: 12.0207\n",
      "Epoch [2/3], Step [2000/3236], Loss: 1.9590, Perplexity: 7.09251\n",
      "Epoch [2/3], Step [2100/3236], Loss: 2.0326, Perplexity: 7.63379\n",
      "Epoch [2/3], Step [2200/3236], Loss: 1.9663, Perplexity: 7.14430\n",
      "Epoch [2/3], Step [2300/3236], Loss: 1.9516, Perplexity: 7.04011\n",
      "Epoch [2/3], Step [2400/3236], Loss: 2.0629, Perplexity: 7.86909\n",
      "Epoch [2/3], Step [2500/3236], Loss: 1.8863, Perplexity: 6.59486\n",
      "Epoch [2/3], Step [2600/3236], Loss: 1.8875, Perplexity: 6.60277\n",
      "Epoch [2/3], Step [2700/3236], Loss: 1.9354, Perplexity: 6.92715\n",
      "Epoch [2/3], Step [2800/3236], Loss: 2.0174, Perplexity: 7.51877\n",
      "Epoch [2/3], Step [2900/3236], Loss: 1.9234, Perplexity: 6.84451\n",
      "Epoch [2/3], Step [3000/3236], Loss: 1.9769, Perplexity: 7.22058\n",
      "Epoch [2/3], Step [3100/3236], Loss: 2.0212, Perplexity: 7.54722\n",
      "Epoch [2/3], Step [3200/3236], Loss: 2.2599, Perplexity: 9.58180\n",
      "Epoch [3/3], Step [100/3236], Loss: 2.0317, Perplexity: 7.626767\n",
      "Epoch [3/3], Step [200/3236], Loss: 2.0528, Perplexity: 7.79008\n",
      "Epoch [3/3], Step [300/3236], Loss: 1.9464, Perplexity: 7.00347\n",
      "Epoch [3/3], Step [400/3236], Loss: 1.9402, Perplexity: 6.96034\n",
      "Epoch [3/3], Step [500/3236], Loss: 2.0270, Perplexity: 7.59124\n",
      "Epoch [3/3], Step [600/3236], Loss: 1.9138, Perplexity: 6.77886\n",
      "Epoch [3/3], Step [700/3236], Loss: 1.9468, Perplexity: 7.00645\n",
      "Epoch [3/3], Step [800/3236], Loss: 2.1516, Perplexity: 8.59824\n",
      "Epoch [3/3], Step [900/3236], Loss: 1.8967, Perplexity: 6.66402\n",
      "Epoch [3/3], Step [1000/3236], Loss: 2.1068, Perplexity: 8.2219\n",
      "Epoch [3/3], Step [1100/3236], Loss: 2.4386, Perplexity: 11.4566\n",
      "Epoch [3/3], Step [1200/3236], Loss: 1.9145, Perplexity: 6.78330\n",
      "Epoch [3/3], Step [1300/3236], Loss: 1.9271, Perplexity: 6.86994\n",
      "Epoch [3/3], Step [1400/3236], Loss: 1.9505, Perplexity: 7.03209\n",
      "Epoch [3/3], Step [1500/3236], Loss: 2.0304, Perplexity: 7.61722\n",
      "Epoch [3/3], Step [1600/3236], Loss: 1.8358, Perplexity: 6.27008\n",
      "Epoch [3/3], Step [1700/3236], Loss: 1.9851, Perplexity: 7.27990\n",
      "Epoch [3/3], Step [1800/3236], Loss: 1.9349, Perplexity: 6.92345\n",
      "Epoch [3/3], Step [1900/3236], Loss: 1.8198, Perplexity: 6.17094\n",
      "Epoch [3/3], Step [2000/3236], Loss: 1.9907, Perplexity: 7.32101\n",
      "Epoch [3/3], Step [2100/3236], Loss: 2.0533, Perplexity: 7.79364\n",
      "Epoch [3/3], Step [2200/3236], Loss: 1.9892, Perplexity: 7.30948\n",
      "Epoch [3/3], Step [2300/3236], Loss: 1.9097, Perplexity: 6.75088\n",
      "Epoch [3/3], Step [2400/3236], Loss: 2.1283, Perplexity: 8.40032\n",
      "Epoch [3/3], Step [2500/3236], Loss: 1.9502, Perplexity: 7.03015\n",
      "Epoch [3/3], Step [2600/3236], Loss: 1.9746, Perplexity: 7.20408\n",
      "Epoch [3/3], Step [2700/3236], Loss: 1.9862, Perplexity: 7.28795\n",
      "Epoch [3/3], Step [2800/3236], Loss: 1.8790, Perplexity: 6.54714\n",
      "Epoch [3/3], Step [2900/3236], Loss: 1.9378, Perplexity: 6.94331\n",
      "Epoch [3/3], Step [3000/3236], Loss: 1.7368, Perplexity: 5.67909\n",
      "Epoch [3/3], Step [3100/3236], Loss: 2.0261, Perplexity: 7.58469\n",
      "Epoch [3/3], Step [3200/3236], Loss: 2.3077, Perplexity: 10.0516\n",
      "Epoch [3/3], Step [3236/3236], Loss: 2.1400, Perplexity: 8.49950"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for i_step in range(1, total_step+1):        \n",
    "\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        \n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved model can be used for inference now."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
